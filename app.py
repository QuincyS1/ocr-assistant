from flask import Flask, request, jsonify, render_template, send_file
import os
import base64
import requests
import json
from docx import Document
from reportlab.lib.pagesizes import letter
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_JUSTIFY
from reportlab.lib.units import inch
import uuid
import re
from difflib import SequenceMatcher
from PIL import Image
import io

app = Flask(__name__)

# Vercelå…¼å®¹æ€§
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
app.config['UPLOAD_FOLDER'] = 'uploads'

# Gemini APIé…ç½®
GEMINI_API_KEY = "AIzaSyDeYj-tpMueljip6MnfjGNjDggllevLjwY"

class OCRProcessor:
    def __init__(self):
        print("åˆå§‹åŒ–OCRå¤„ç†å™¨")
    
    def ocr_image(self, image_data):
        # ä½¿ç”¨Gemini APIè¿›è¡ŒOCRè¯†åˆ«
        gemini_result = self._try_gemini_ocr(image_data)
        if not gemini_result.startswith("APIé”™è¯¯") and not gemini_result.startswith("OCRè¯†åˆ«é”™è¯¯"):
            return gemini_result
        
        # Geminiå¤±è´¥æ—¶è¿”å›æç¤ºä¿¡æ¯
        return "å›¾ç‰‡è¯†åˆ«å¤±è´¥ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIé…ç½®ã€‚æ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬å†…å®¹è¿›è¡Œæ•´ç†å’Œå¯¼å‡ºã€‚"
    
    def _try_gemini_ocr(self, image_data):
        if not GEMINI_API_KEY or GEMINI_API_KEY == 'your_gemini_api_key':
            return "APIé”™è¯¯: æœªé…ç½®Gemini APIå¯†é’¥"
        
        img_base64 = base64.b64encode(image_data).decode('utf-8')
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "contents": [{
                "parts": [
                    {
                        "text": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚é‡è¦è¦æ±‚ï¼š1)ä¿æŒåŸå›¾ç‰‡çš„æ®µè½ç»“æ„ï¼Œæ®µè½ä¹‹é—´ç”¨åŒæ¢è¡Œåˆ†éš”ã€‚2)å°†åŒä¸€æ®µè½å†…çš„æ¢è¡Œåˆå¹¶ä¸ºè¿ç»­æ–‡æœ¬ï¼Œè‹±æ–‡å•è¯é—´åŠ ç©ºæ ¼ï¼Œä¸­æ–‡å­—ç¬¦ç›´æ¥è¿æ¥ã€‚3)åªè¾“å‡ºæ•´ç†åçš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜ã€‚"
                    },
                    {
                        "inline_data": {
                            "mime_type": "image/jpeg",
                            "data": img_base64
                        }
                    }
                ]
            }]
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code != 200:
                return f"APIé”™è¯¯: HTTP {response.status_code}"
            
            result = response.json()
            
            if 'candidates' in result and len(result['candidates']) > 0:
                text = result['candidates'][0]['content']['parts'][0]['text']
                print(f"Geminiè¯†åˆ«æˆåŠŸ")
                return text
            elif 'error' in result:
                return f"APIé”™è¯¯: {result['error']['message']}"
            return "APIé”™è¯¯: æœªè¯†åˆ«åˆ°æ–‡å­—"
        except Exception as e:
            return f"OCRè¯†åˆ«é”™è¯¯: {str(e)}"
    


class TextProcessor:

    
    @staticmethod
    def clean_noise_text(text):
        import re
        
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # åªåˆ é™¤æ˜æ˜¾çš„å¹¿å‘Šè¯
            if any(keyword in line for keyword in ["ä¸‹è½½åº”ç”¨", "ç«‹å³ä¸‹è½½", "æ‰«ç ä¸‹è½½", "ç‚¹å‡»ä¸‹è½½", "ç«‹å³åŠ å…¥", "å…è´¹ä¼šå‘˜", "å¼€é€šä¼šå‘˜"]):
                continue
            
            # åˆ é™¤æ—¶é—´æˆ³ (å¦‚: 2023-10-26 13:40:12, 13:40, 10æœˆ26æ—¥)
            if re.match(r'^\d{1,4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?\s*\d{1,2}[æ—¶:]\d{1,2}', line):
                continue
            if re.match(r'^\d{1,2}[æ—¶:]\d{1,2}', line) and len(line) < 10:
                continue
            
            # åˆ é™¤é¡µç  (å¦‚: ç¬¬1é¡µ, 1/10, Page 1)
            if re.match(r'^(ç¬¬\d+é¡µ|\d+/\d+|Page\s*\d+)$', line):
                continue
            
            clean_lines.append(line)
        
        return '\n'.join(clean_lines)
    
    @staticmethod
    def merge_ocr_results(results):
        """æ”¹è¿›çš„OCRç»“æœåˆå¹¶ï¼Œæ”¯æŒé•¿æ–‡æœ¬é‡å¤æ£€æµ‹å’Œå»é‡"""
        if len(results) <= 1:
            return results[0]['processed_text'] if results else ""
        
        # æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
        pages = [result['processed_text'] for result in results]
        
        # æ¸…ç†æ¯é¡µæ–‡æœ¬
        cleaned_pages = []
        for page in pages:
            # ç§»é™¤ä¸­æ–‡é—´ç©ºæ ¼
            cleaned = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', page)
            cleaned_pages.append(cleaned.strip())
        
        # æ£€æµ‹å’Œåˆå¹¶é‡å¤å†…å®¹
        merged_pages = TextProcessor.detect_and_merge_overlaps(cleaned_pages)
        
        # æœ€ç»ˆæ¸…ç†
        return TextProcessor.final_cleanup(merged_pages)
    
    @staticmethod
    def detect_and_merge_overlaps(pages):
        """æ£€æµ‹å¹¶åˆå¹¶é‡å¤å†…å®¹"""
        if len(pages) <= 1:
            return '\n\n'.join(pages)
        
        # æ„å»ºé‡å¤å…³ç³»å›¾
        overlap_graph = {}
        for i in range(len(pages)):
            for j in range(len(pages)):
                if i != j:
                    overlap_info = TextProcessor.find_overlap(pages[i], pages[j])
                    if overlap_info['has_overlap']:
                        overlap_graph[i] = {'target': j, 'info': overlap_info}
        
        # æŒ‰é¡ºåºåˆå¹¶
        merged_content = []
        used_pages = set()
        
        for i in range(len(pages)):
            if i in used_pages:
                continue
                
            current_page = pages[i]
            used_pages.add(i)
            
            # æŸ¥æ‰¾ä¸å½“å‰é¡µé‡å¤çš„é¡µé¢
            if i in overlap_graph:
                target_idx = overlap_graph[i]['target']
                overlap_info = overlap_graph[i]['info']
                
                if target_idx not in used_pages:
                    # åˆå¹¶ä¸¤é¡µ
                    merged_page = TextProcessor.merge_two_overlapping_pages(
                        current_page, pages[target_idx], overlap_info
                    )
                    merged_content.append(merged_page)
                    used_pages.add(target_idx)
                else:
                    merged_content.append(current_page)
            else:
                merged_content.append(current_page)
        
        return '\n\n'.join(merged_content)
    
    @staticmethod
    def find_overlap(page1, page2):
        """ä½¿ç”¨SequenceMatcheræ£€æµ‹ä¸¤é¡µé—´çš„é‡å¤"""
        # å°†æ–‡æœ¬åˆ†è§£ä¸ºå¥å­
        sentences1 = re.split(r'[\u3002ï¼ï¼Ÿ.!?]', page1)
        sentences2 = re.split(r'[\u3002ï¼ï¼Ÿ.!?]', page2)
        
        # æ¸…ç†ç©ºå¥å­
        sentences1 = [s.strip() for s in sentences1 if s.strip()]
        sentences2 = [s.strip() for s in sentences2 if s.strip()]
        
        # æ£€æµ‹æœ€é•¿å…¬å…±å­åºåˆ—
        matcher = SequenceMatcher(None, sentences1, sentences2)
        match = matcher.find_longest_match(0, len(sentences1), 0, len(sentences2))
        
        if match.size > 0:
            # è®¡ç®—é‡å¤å†…å®¹çš„å­—ç¬¦æ•°
            overlap_sentences = sentences1[match.a:match.a + match.size]
            overlap_text = ''.join(overlap_sentences)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ„ä¹‰çš„é‡å¤
            if len(overlap_text) >= 20 and match.size >= 1:
                return {
                    'has_overlap': True,
                    'page1_start': match.a,
                    'page1_end': match.a + match.size,
                    'page2_start': match.b,
                    'page2_end': match.b + match.size,
                    'overlap_length': len(overlap_text)
                }
        
        return {'has_overlap': False}
    
    @staticmethod
    def merge_two_overlapping_pages(page1, page2, overlap_info):
        """åˆå¹¶ä¸¤ä¸ªæœ‰é‡å¤çš„é¡µé¢"""
        sentences1 = re.split(r'[\u3002ï¼ï¼Ÿ.!?]', page1)
        sentences2 = re.split(r'[\u3002ï¼ï¼Ÿ.!?]', page2)
        
        sentences1 = [s.strip() for s in sentences1 if s.strip()]
        sentences2 = [s.strip() for s in sentences2 if s.strip()]
        
        # å–page1çš„éé‡å¤éƒ¨åˆ† + page2çš„éé‡å¤éƒ¨åˆ†
        result_sentences = []
        
        # æ·»åŠ page1çš„å‰éƒ¨åˆ†
        result_sentences.extend(sentences1[:overlap_info['page1_end']])
        
        # æ·»åŠ page2çš„åéƒ¨åˆ†ï¼ˆå»é™¤é‡å¤éƒ¨åˆ†ï¼‰
        result_sentences.extend(sentences2[overlap_info['page2_end']:])
        
        # é‡æ–°ç»„åˆæˆæ®µè½
        return 'ã€‚'.join(result_sentences) + 'ã€‚' if result_sentences else ''
    
    @staticmethod
    def final_cleanup(text):
        """æœ€ç»ˆæ¸…ç†åˆå¹¶åçš„æ–‡æœ¬ - ä¿æŒæ®µè½ç»“æ„"""
        if not text:
            return ""
        
        # ç§»é™¤ä¸­æ–‡é—´ç©ºæ ¼
        text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
        
        # æŒ‰æ®µè½å¤„ç†ï¼ˆä¿æŒåŒæ¢è¡Œåˆ†éš”ï¼‰
        paragraphs = text.split('\n\n')
        clean_paragraphs = []
        seen_paragraphs = set()
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if paragraph and paragraph not in seen_paragraphs:
                clean_paragraphs.append(paragraph)
                seen_paragraphs.add(paragraph)
        
        return '\n\n'.join(clean_paragraphs)
    
    @staticmethod
    def is_continuous(text1, text2):
        """åˆ¤æ–­ä¸¤é¡µæ˜¯å¦è¿ç»­"""
        lines1 = [line.strip() for line in text1.split('\n') if line.strip()]
        lines2 = [line.strip() for line in text2.split('\n') if line.strip()]
        
        if not lines1 or not lines2:
            return False
        
        # æ£€æŸ¥text2çš„å‰ä¸¤è¡Œæ˜¯å¦åœ¨text1ä¸­å‡ºç°
        for line2 in lines2[:2]:
            for line1 in lines1:
                if line2 in line1 or line1 in line2:
                    return True
        
        # æ£€æŸ¥text1çš„å‰ä¸¤è¡Œæ˜¯å¦åœ¨text2ä¸­å‡ºç°
        for line1 in lines1[:2]:
            for line2 in lines2:
                if line1 in line2 or line2 in line1:
                    return True
        
        return False
    
    @staticmethod
    def reorder_pages(connections, n):
        """æ ¹æ®è¿ç»­æ€§å…³ç³»é‡æ’åºé¡µé¢"""
        if not connections:
            return list(range(n))
        
        # æ‰¾åˆ°èµ·å§‹é¡µï¼ˆæ²¡æœ‰è¢«å…¶ä»–é¡µæŒ‡å‘çš„é¡µï¼‰
        pointed_to = set(connections.values())
        start_pages = [i for i in range(n) if i not in pointed_to]
        
        if not start_pages:
            start_pages = [0]  # é»˜è®¤ä»ç¬¬ä¸€é¡µå¼€å§‹
        
        # æŒ‰è¿ç»­æ€§é“¾æ’åº
        ordered = []
        visited = set()
        
        for start in start_pages:
            current = start
            while current is not None and current not in visited:
                ordered.append(current)
                visited.add(current)
                current = connections.get(current)
        
        # æ·»åŠ æœªè®¿é—®çš„é¡µé¢
        for i in range(n):
            if i not in visited:
                ordered.append(i)
        
        return ordered
    
    @staticmethod
    def smart_merge(text1, text2):
        """æ™ºèƒ½åˆå¹¶ä¸¤æ®µæ–‡æœ¬ï¼Œå»é™¤é‡å¤"""
        lines1 = [line.strip() for line in text1.split('\n') if line.strip()]
        lines2 = [line.strip() for line in text2.split('\n') if line.strip()]
        
        # æ‰¾åˆ°é‡å¤çš„èµ·å§‹ä½ç½®
        start_idx = 0
        for i, line2 in enumerate(lines2):
            found_overlap = False
            for line1 in lines1[-3:]:  # æ£€æŸ¥text1çš„æœ€å3è¡Œ
                if line2 in line1 or line1 in line2:
                    start_idx = i + 1
                    found_overlap = True
                    break
            if not found_overlap:
                break
        
        return '\n'.join(lines2[start_idx:])
    
    @staticmethod
    def clean_merged_text(text):
        """æ¸…ç†åˆå¹¶åçš„æ–‡æœ¬ï¼Œå»é™¤é‡å¤è¡Œå’Œç©ºè¡Œ"""
        lines = text.split('\n')
        clean_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen_lines:
                clean_lines.append(line)
                seen_lines.add(line)
        
        # é‡æ–°ç»„ç»‡æ®µè½
        paragraphs = []
        current_paragraph = []
        
        for line in clean_lines:
            if line.endswith(('ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ')):
                current_paragraph.append(line)
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
            else:
                current_paragraph.append(line)
        
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        return '\n\n'.join(paragraphs)
    
    @staticmethod
    def find_text_overlap(text1, text2):
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„é‡å¤ç‡"""
        lines1 = text1.split('\n')[-3:]  # å–æœ€å3è¡Œ
        lines2 = text2.split('\n')[:3]   # å–å‰3è¡Œ
        
        overlap_count = 0
        total_lines = len(lines2)
        
        for line2 in lines2:
            for line1 in lines1:
                if line2.strip() and line1.strip() and line2.strip() in line1.strip():
                    overlap_count += 1
                    break
        
        return overlap_count / total_lines if total_lines > 0 else 0
    
    @staticmethod
    def remove_overlap(text1, text2):
        """ä»ç¬¬äºŒæ®µæ–‡æœ¬ä¸­ç§»é™¤ä¸ç¬¬ä¸€æ®µé‡å¤çš„éƒ¨åˆ†"""
        lines1 = text1.split('\n')
        lines2 = text2.split('\n')
        
        # æ‰¾åˆ°é‡å¤çš„èµ·å§‹ä½ç½®
        start_idx = 0
        for i, line2 in enumerate(lines2):
            if any(line2.strip() in line1 for line1 in lines1[-3:] if line1.strip()):
                start_idx = i + 1
            else:
                break
        
        return '\n'.join(lines2[start_idx:])
    
    @staticmethod
    def llm_content_filter(text):
        """ä½¿ç”¨Geminiè¿›ä¸€æ­¥è¿‡æ»¤éæ­£æ–‡å†…å®¹"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ­£æ–‡å†…å®¹ï¼Œå»é™¤å¹¿å‘Šã€æŒ‰é’®ã€å¯¼èˆªç­‰éæ­£æ–‡å…ƒç´ ï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡ç« å†…å®¹ï¼š\n\n{text}"
                }]
            }]
        }
        
        try:
            response = requests.post(url, headers={"Content-Type": "application/json"}, json=payload)
            result = response.json()
            if 'candidates' in result and len(result['candidates']) > 0:
                return result['candidates'][0]['content']['parts'][0]['text']
        except:
            pass
        return text
    
    @staticmethod
    def remove_chinese_spaces(text):
        """ç§»é™¤ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„å¤šä½™ç©ºæ ¼"""
        import re
        # ç§»é™¤ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼
        text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
        # ç§»é™¤ä¸­æ–‡ä¸æ ‡ç‚¹ä¹‹é—´çš„ç©ºæ ¼
        text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])', '', text)
        text = re.sub(r'(?<=[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])\s+(?=[\u4e00-\u9fff])', '', text)
        return text
    
    @staticmethod
    def clean_and_merge_texts(pages):
        """æ•´åˆç©ºæ ¼æ¸…ç†ä¸å¤šé¡µæ‹¼æ¥çš„ä¸»å‡½æ•°"""
        if not pages:
            return ""
        
        # 1. å¯¹æ¯é¡µè¿›è¡ŒåŸºç¡€æ¸…ç†å’Œç©ºæ ¼å¤„ç†
        cleaned_pages = []
        for page_text in pages:
            # åŸºç¡€å™ªå£°æ¸…ç†
            cleaned = TextProcessor.clean_noise_text(page_text)
            # ç§»é™¤ä¸­æ–‡é—´å¤šä½™ç©ºæ ¼
            cleaned = TextProcessor.remove_chinese_spaces(cleaned)
            if cleaned.strip():
                cleaned_pages.append(cleaned)
        
        if not cleaned_pages:
            return ""
        
        if len(cleaned_pages) == 1:
            return TextProcessor.format_final_text(cleaned_pages[0])
        
        # 2. æ£€æµ‹é¡µé¢è¿ç»­æ€§å¹¶é‡æ’åº
        ordered_pages = TextProcessor.detect_and_reorder_pages(cleaned_pages)
        
        # 3. æ™ºèƒ½æ‹¼æ¥å»é‡
        merged_text = TextProcessor.smart_merge_pages(ordered_pages)
        
        # 4. æœ€ç»ˆæ ¼å¼åŒ–
        return TextProcessor.format_final_text(merged_text)
    
    @staticmethod
    def detect_and_reorder_pages(pages):
        """æ£€æµ‹é¡µé¢è¿ç»­æ€§å¹¶é‡æ–°æ’åº"""
        n = len(pages)
        if n <= 1:
            return pages
        
        # æ„å»ºè¿ç»­æ€§å…³ç³»å›¾
        connections = {}
        for i in range(n):
            for j in range(n):
                if i != j and TextProcessor.pages_are_continuous(pages[i], pages[j]):
                    connections[i] = j
        
        if not connections:
            return pages  # æ— è¿ç»­å…³ç³»ï¼Œä¿æŒåŸé¡ºåº
        
        # æ‰¾åˆ°èµ·å§‹é¡µé¢ï¼ˆæ²¡æœ‰è¢«å…¶ä»–é¡µé¢æŒ‡å‘ï¼‰
        pointed_to = set(connections.values())
        start_pages = [i for i in range(n) if i not in pointed_to]
        
        if not start_pages:
            start_pages = [0]  # å¦‚æœå½¢æˆç¯ï¼Œä»ç¬¬ä¸€é¡µå¼€å§‹
        
        # æŒ‰è¿ç»­æ€§é“¾é‡æ’åº
        ordered_indices = []
        visited = set()
        
        for start in start_pages:
            current = start
            while current is not None and current not in visited:
                ordered_indices.append(current)
                visited.add(current)
                current = connections.get(current)
        
        # æ·»åŠ æœªè®¿é—®çš„é¡µé¢
        for i in range(n):
            if i not in visited:
                ordered_indices.append(i)
        
        return [pages[i] for i in ordered_indices]
    
    @staticmethod
    def pages_are_continuous(page1, page2):
        """åˆ¤æ–­ä¸¤é¡µæ˜¯å¦è¿ç»­"""
        lines1 = [line.strip() for line in page1.split('\n') if line.strip()]
        lines2 = [line.strip() for line in page2.split('\n') if line.strip()]
        
        if not lines1 or not lines2:
            return False
        
        # æ£€æŸ¥page2çš„å‰ä¸¤è¡Œæ˜¯å¦åœ¨page1çš„ä»»æ„è¡Œä¸­å®Œå…¨åŒ¹é…
        for line2 in lines2[:2]:
            if any(line2 == line1 for line1 in lines1):
                return True
        
        # æ£€æŸ¥page1çš„å‰ä¸¤è¡Œæ˜¯å¦åœ¨page2çš„ä»»æ„è¡Œä¸­å®Œå…¨åŒ¹é…
        for line1 in lines1[:2]:
            if any(line1 == line2 for line2 in lines2):
                return True
        
        return False
    
    @staticmethod
    def smart_merge_pages(pages):
        """æ™ºèƒ½æ‹¼æ¥å¤šé¡µï¼Œå»é™¤é‡å¤å†…å®¹"""
        if not pages:
            return ""
        
        merged_text = pages[0]
        
        for i in range(1, len(pages)):
            current_page = pages[i]
            merged_text = TextProcessor.merge_two_pages(merged_text, current_page)
        
        return merged_text
    
    @staticmethod
    def merge_two_pages(text1, text2):
        """åˆå¹¶ä¸¤é¡µæ–‡æœ¬ï¼Œå»é™¤é‡å¤éƒ¨åˆ†"""
        # æŒ‰å¥å­åˆ†å‰²æ¥æ£€æµ‹é‡å¤
        sentences1 = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text1)
        sentences2 = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text2)
        
        # æ¸…ç†ç©ºå¥å­
        sentences1 = [s.strip() for s in sentences1 if s.strip()]
        sentences2 = [s.strip() for s in sentences2 if s.strip()]
        
        # æ‰¾åˆ°é‡å¤çš„èµ·å§‹ä½ç½®
        overlap_start = 0
        for i, sent2 in enumerate(sentences2):
            # æ£€æŸ¥è¿™ä¸ªå¥å­æ˜¯å¦åœ¨text1çš„åå‡ ä¸ªå¥å­ä¸­å‡ºç°
            found_in_text1 = False
            for sent1 in sentences1[-3:]:
                if sent2 in sent1 or sent1 in sent2 or sent2 == sent1:
                    found_in_text1 = True
                    break
            
            if found_in_text1:
                overlap_start = i + 1
            else:
                break
        
        # æ‹¼æ¥éé‡å¤éƒ¨åˆ†
        unique_sentences2 = sentences2[overlap_start:]
        if unique_sentences2:
            # é‡æ–°ç»„åˆæˆæ®µè½
            all_sentences = sentences1 + unique_sentences2
            return 'ã€‚'.join(all_sentences) + 'ã€‚' if all_sentences else ''
        else:
            return 'ã€‚'.join(sentences1) + 'ã€‚' if sentences1 else ''
    
    @staticmethod
    def format_final_text(text):
        """æœ€ç»ˆæ–‡æœ¬æ ¼å¼åŒ–å¤„ç† - æ™ºèƒ½åˆå¹¶æ¢è¡Œä¿æŒæ®µè½"""
        if not text:
            return ""
        
        # å…ˆç§»é™¤ä¸­æ–‡é—´å¤šä½™ç©ºæ ¼
        text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
        
        # æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        paragraphs = re.split(r'\n\s*\n', text)
        formatted_paragraphs = []
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
            
            # å¯¹æ¯ä¸ªæ®µè½å†…éƒ¨å¤„ç†
            lines = paragraph.split('\n')
            cleaned_lines = []
            
            for line in lines:
                line = line.strip()
                if line:
                    cleaned_lines.append(line)
            
            if cleaned_lines:
                # æ™ºèƒ½åˆå¹¶ï¼šè‹±æ–‡åŠ ç©ºæ ¼ï¼Œä¸­æ–‡ç›´æ¥è¿æ¥
                merged_text = ""
                for i, line in enumerate(cleaned_lines):
                    if i == 0:
                        merged_text = line
                    else:
                        # å¦‚æœå‰ä¸€è¡Œä»¥è‹±æ–‡ç»“å°¾ä¸”å½“å‰è¡Œä»¥è‹±æ–‡å¼€å¤´ï¼ŒåŠ ç©ºæ ¼
                        prev_line = cleaned_lines[i-1]
                        if (re.search(r'[a-zA-Z]$', prev_line) and 
                            re.search(r'^[a-zA-Z]', line)):
                            merged_text += " " + line
                        else:
                            merged_text += line
                
                formatted_paragraphs.append(merged_text)
        
        return '\n\n'.join(formatted_paragraphs)
    
    @staticmethod
    def clean_and_format(text):
        """å•é¡µæ–‡æœ¬æ¸…ç†å’Œæ ¼å¼åŒ– - å¼ºåˆ¶æ®µè½å¤„ç†"""
        if not text:
            return ""
        
        # å…ˆè¿›è¡ŒåŸºç¡€æ¸…ç†
        cleaned_text = TextProcessor.clean_noise_text(text)
        
        # å¼ºåˆ¶æ®µè½å¤„ç†
        return TextProcessor.force_paragraph_formatting(cleaned_text)
    
    @staticmethod
    def force_paragraph_formatting(text):
        """å¼ºåˆ¶æ®µè½æ ¼å¼åŒ– - æ™ºèƒ½æ£€æµ‹æ®µè½"""
        if not text:
            return ""
        
        lines = text.split('\n')
        paragraphs = []
        current_paragraph = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                # ç©ºè¡Œè¡¨ç¤ºæ®µè½ç»“æŸ
                if current_paragraph:
                    paragraphs.append(current_paragraph)
                    current_paragraph = []
            else:
                # æ™ºèƒ½æ£€æµ‹æ®µè½åˆ†éš”
                should_start_new_paragraph = False
                
                if current_paragraph:
                    prev_line = current_paragraph[-1]
                    
                    # æ£€æµ‹æ®µè½ç»“æŸæ ‡å¿—
                    if (prev_line.endswith(('ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ')) and
                        not line.startswith(('ä½†æ˜¯', 'ç„¶è€Œ', 'å› æ­¤', 'æ‰€ä»¥', 'However', 'But', 'Therefore'))):
                        should_start_new_paragraph = True
                    
                    # æ£€æµ‹ç¼©è¿›æˆ–ç‰¹æ®Šæ ¼å¼
                    if (line.startswith(('    ', '\t', 'ã€€ã€€')) or
                        re.match(r'^\d+[\.ï¼‰]', line) or  # ç¼–å·åˆ—è¡¨
                        re.match(r'^[A-Za-z]\)', line)):   # å­—æ¯åˆ—è¡¨
                        should_start_new_paragraph = True
                
                if should_start_new_paragraph:
                    paragraphs.append(current_paragraph)
                    current_paragraph = [line]
                else:
                    current_paragraph.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph:
            paragraphs.append(current_paragraph)
        
        # åˆå¹¶æ¯ä¸ªæ®µè½å†…çš„è¡Œ
        formatted_paragraphs = []
        for paragraph_lines in paragraphs:
            if not paragraph_lines:
                continue
            
            # æ™ºèƒ½åˆå¹¶è¡Œ
            merged_text = ""
            for i, line in enumerate(paragraph_lines):
                if i == 0:
                    merged_text = line
                else:
                    # è‹±æ–‡å•è¯é—´åŠ ç©ºæ ¼ï¼Œä¸­æ–‡ç›´æ¥è¿æ¥
                    prev_line = paragraph_lines[i-1]
                    if (re.search(r'[a-zA-Z]$', prev_line) and 
                        re.search(r'^[a-zA-Z]', line)):
                        merged_text += " " + line
                    else:
                        merged_text += line
            
            formatted_paragraphs.append(merged_text)
        
        # ç”¨åŒæ¢è¡Œåˆ†éš”æ®µè½
        return '\n\n'.join(formatted_paragraphs)
    
    @staticmethod
    def llm_semantic_correction(text):
        """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ä¿®æ­£ï¼Œä¿ç•™è¯­ä¹‰è¿è´¯çš„å¥å­"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": f"è¯·ä¿®æ­£ä»¥ä¸‹æ–‡æœ¬ï¼Œä¿ç•™è¯­ä¹‰è¿è´¯çš„å¥å­ï¼Œåˆ é™¤ä¸å®Œæ•´æˆ–æ— æ„ä¹‰çš„ç‰‡æ®µï¼Œä½¿æ–‡æœ¬æ›´åŠ æµç•…è‡ªç„¶ï¼š\n\n{text}"
                }]
            }]
        }
        
        try:
            response = requests.post(url, headers={"Content-Type": "application/json"}, json=payload)
            result = response.json()
            if 'candidates' in result and len(result['candidates']) > 0:
                return result['candidates'][0]['content']['parts'][0]['text']
        except:
            pass
        return text

    @staticmethod
    def dev_clean_text(ocr_text: str) -> str:
        """
        ğŸ§  å¼€å‘è€…å‘½ä»¤ï¼šæ¸…ç†OCRæ¢è¡Œç¬¦ï¼Œåªä¿ç•™æ®µè½ç©ºè¡Œã€‚
        åŠŸèƒ½ï¼š
        1. åˆ é™¤æ®µå†…æ¢è¡Œï¼ˆåˆå¹¶ä¸ºä¸€è¡Œï¼‰
        2. ä¿ç•™æ®µè½ç©ºè¡Œï¼ˆåŒæ¢è¡Œï¼‰
        3. å»æ‰å¤šä½™ç©ºæ ¼
        """
        if not ocr_text:
            return ""
        
        text = ocr_text.replace("\r\n", "\n").strip()
        # åˆå¹¶è¡Œå†…æ¢è¡Œï¼Œåªä¿ç•™æ®µè½ç©ºè¡Œ
        text = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        text = re.sub(r'[ \t]+', ' ', text)
        # æŒ‰åŒæ¢è¡Œåˆ†æ®µï¼Œå»æ‰æ®µé¦–å°¾ç©ºæ ¼
        paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
        return "\n\n".join(paragraphs)


class FileExporter:
    @staticmethod
    def export_txt(text, filename):
        filepath = f"/tmp/{filename}.txt"
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(text)
        return filepath
    
    @staticmethod
    def export_docx(text, filename):
        doc = Document()
        for paragraph in text.split('\n\n'):
            doc.add_paragraph(paragraph)
        
        filepath = f"/tmp/{filename}.docx"
        doc.save(filepath)
        return filepath
    
    @staticmethod
    def export_md(text, filename):
        md_content = text.replace('\n\n', '\n\n')
        filepath = f"/tmp/{filename}.md"
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        return filepath
    
    @staticmethod
    def export_pdf(text, filename):
        filepath = f"/tmp/{filename}.pdf"
        
        # æ³¨å†Œä¸­æ–‡å­—ä½“
        font_name = 'NotoSans'
        font_registered = False
        
        # å°è¯•å¤šä¸ªå­—ä½“è·¯å¾„
        font_paths = [
            'fonts/NotoSansCJK.ttc',
            'fonts/NotoSansCJKsc-Regular.otf',
            'fonts/NotoSansCJKsc-Regular.ttf',
            '/Users/quincys/Library/Fonts/NotoSansCJK.ttc',
            '/opt/homebrew/Caskroom/font-noto-sans-cjk/2.004/NotoSansCJK.ttc',
            'static/fonts/NotoSansCJKsc-Regular.ttf'
        ]
        
        for font_path in font_paths:
            try:
                if os.path.exists(font_path):
                    # å¯¹äº.ttcæ–‡ä»¶ï¼ŒæŒ‡å®šä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ä½“
                    if font_path.endswith('.ttc'):
                        pdfmetrics.registerFont(TTFont(font_name, font_path, subfontIndex=0))
                    else:
                        pdfmetrics.registerFont(TTFont(font_name, font_path))
                    font_registered = True
                    print(f"æˆåŠŸæ³¨å†Œå­—ä½“: {font_path}")
                    break
            except Exception as e:
                print(f"æ³¨å†Œå­—ä½“å¤±è´¥ {font_path}: {e}")
                continue
        
        if not font_registered:
            # å°è¯•ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸­æ–‡å­—ä½“
            try:
                from reportlab.pdfbase.cidfonts import UnicodeCIDFont
                pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
                font_name = 'STSong-Light'
                font_registered = True
                print("ä½¿ç”¨CIDå­—ä½“: STSong-Light")
            except:
                font_name = 'Helvetica'
                print("è­¦å‘Š: æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        
        # åˆ›å»ºPDFæ–‡æ¡£
        doc = SimpleDocTemplate(filepath, pagesize=letter,
                              rightMargin=72, leftMargin=72,
                              topMargin=72, bottomMargin=18)
        
        # åˆ›å»ºæ®µè½æ ·å¼
        styles = getSampleStyleSheet()
        chinese_style = ParagraphStyle(
            'ChineseStyle',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=11,
            leading=16,
            alignment=TA_JUSTIFY,
            spaceBefore=6,
            spaceAfter=6,
            leftIndent=0,
            rightIndent=0
        )
        
        # æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºæ ¼
        cleaned_text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
        
        # æ„å»ºPDFå†…å®¹ - ä¿æŒæ®µè½ç»“æ„
        story = []
        
        # æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        for paragraph_text in cleaned_text.split('\n\n'):
            if paragraph_text.strip():
                # è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
                paragraph_text = paragraph_text.replace('&', '&amp;')
                paragraph_text = paragraph_text.replace('<', '&lt;')
                paragraph_text = paragraph_text.replace('>', '&gt;')
                
                para = Paragraph(paragraph_text, chinese_style)
                story.append(para)
                story.append(Spacer(1, 12))  # æ®µè½é—´è·
        
        # ç”ŸæˆPDF
        doc.build(story)
        return filepath

ocr_processor = OCRProcessor()
text_processor = TextProcessor()
file_exporter = FileExporter()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_files():
    if 'files' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    files = request.files.getlist('files')
    results = []
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆæ”¯æŒè¿ç»­æˆªå›¾ï¼‰
    sorted_files = sorted(files, key=lambda f: f.filename)
    
    for file in sorted_files:
        if file.filename == '':
            continue
        
        # è¯»å–å›¾ç‰‡æ•°æ®
        image_data = file.read()
        
        # OCRè¯†åˆ«
        raw_text = ocr_processor.ocr_image(image_data)
        
        # AIæ–‡æœ¬å¤„ç†ï¼ˆå¢å¼ºæ¸…ç†ï¼‰
        # å…ˆç”¨å¼€å‘è€…æ¸…æ´—å‘½ä»¤å»é™¤å¤šä½™æ¢è¡Œï¼Œå†åšæ ¼å¼åŒ–
        cleaned_text = text_processor.dev_clean_text(raw_text)
        processed_text = text_processor.clean_and_format(cleaned_text)

        
        results.append({
            'filename': file.filename,
            'raw_text': raw_text,
            'processed_text': processed_text
        })
    
    # æ£€æŸ¥æ˜¯å¦æœ‰OCRå¤±è´¥çš„ç»“æœ
    failed_results = [r for r in results if r['raw_text'].startswith(('æ— æ³•è¿æ¥OCRæœåŠ¡', 'APIé”™è¯¯', 'OCRè¯†åˆ«é”™è¯¯'))]
    if failed_results:
        # è¿”å›éƒ¨åˆ†æˆåŠŸçš„ç»“æœï¼Œå³ä½¿æœ‰å¤±è´¥
        successful_results = [r for r in results if not r['raw_text'].startswith(('æ— æ³•è¿æ¥OCRæœåŠ¡', 'APIé”™è¯¯', 'OCRè¯†åˆ«é”™è¯¯'))]
        if successful_results:
            page_texts = [result['processed_text'] for result in successful_results]
            merged_text = text_processor.clean_and_merge_texts(page_texts)
            return jsonify({
                'results': results,
                'merged_text': merged_text,
                'warning': f'{len(failed_results)}å¼ å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼Œå·²å¤„ç†{len(successful_results)}å¼ æˆåŠŸçš„å›¾ç‰‡'
            })
        else:
            return jsonify({
                'error': 'æ‰€æœ‰å›¾ç‰‡OCRè¯†åˆ«å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é…ç½®APIå¯†é’¥',
                'results': results
            }), 400
    
    # ä½¿ç”¨æ–°çš„æ¸…ç†å’Œåˆå¹¶å‡½æ•°
    page_texts = [result['processed_text'] for result in results]
    merged_text = text_processor.clean_and_merge_texts(page_texts)
    
    # å¯é€‰ï¼šè¿›è¡Œè¯­ä¹‰ä¿®æ­£
    if len(merged_text) > 1000:
        merged_text = text_processor.llm_semantic_correction(merged_text)
    
    # è¿”å›åˆå¹¶åçš„ç»“æœ
    return jsonify({
        'results': results,
        'merged_text': merged_text
    })
    
    return jsonify({'results': results})

@app.route('/dev/clean', methods=['POST'])
def dev_clean():
    """
    ğŸ§¹ å¼€å‘è€…æ¥å£ï¼šæµ‹è¯• OCR æ–‡æœ¬çš„æ¢è¡Œæ¸…ç†
    ç”¨æ³•ï¼š
        POST { "text": "OCRåŸæ–‡" }
    è¿”å›ï¼š
        {"cleaned_text": "..."}
    """
    data = request.get_json()
    text = data.get('text', '')
    cleaned = text_processor.dev_clean_text(text)
    return jsonify({"cleaned_text": cleaned})




@app.route('/export', methods=['POST'])
def export_file():
    data = request.json
    text = data.get('text', '')
    format_type = data.get('format', 'txt')
    filename = data.get('filename', f'ocr_result_{uuid.uuid4().hex[:8]}')
    
    try:
        if format_type == 'txt':
            filepath = file_exporter.export_txt(text, filename)
        elif format_type == 'docx':
            filepath = file_exporter.export_docx(text, filename)
        elif format_type == 'md':
            filepath = file_exporter.export_md(text, filename)
        elif format_type == 'pdf':
            filepath = file_exporter.export_pdf(text, filename)
        else:
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400
        
        return send_file(filepath, as_attachment=True)
    
    except Exception as e:
        return jsonify({'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500

# Verceléœ€è¦å¯¼å‡ºappå¯¹è±¡
if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5005)